{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Dynamical Systems\n",
    "\n",
    "This notebooks contains the experiments to evaluate graph edit networks on simple graph dynamical systems, namely the edit cycles, degree rules, and game of life datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_graph_edit_networks as gen\n",
    "import baseline_models\n",
    "import hep_th\n",
    "\n",
    "# model hyperparameters\n",
    "num_layers = 2\n",
    "dim_hid = 64\n",
    "\n",
    "# training hyperparameters\n",
    "learning_rate  = 1E-3\n",
    "weight_decay   = 1E-5\n",
    "loss_threshold = 1E-3\n",
    "max_epochs     = 50000\n",
    "print_step     = 1000\n",
    "\n",
    "# the number of repitions for each experiment\n",
    "R = 5\n",
    "# the number of test time series we use to evaluate learning afterwards\n",
    "N_test = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model names\n",
    "models = ['GEN', 'GEN_crossent', 'VGAE']\n",
    "\n",
    "# set up functions to initialize the models\n",
    "def setup_vgae(dim_in, nonlin):\n",
    "    return baseline_models.VGAE(num_layers = num_layers, dim_in = dim_in, dim_hid = dim_hid, beta = 1E-3, sigma_scaling = 1E-3, nonlin = nonlin)\n",
    "def setup_gen(dim_in, nonlin):\n",
    "    return gen.GEN(num_layers = num_layers, dim_in = dim_in, dim_hid = dim_hid, nonlin = nonlin)\n",
    "setup_funs = [setup_gen, setup_gen, setup_vgae]\n",
    "# set up functions to compute the loss\n",
    "loss_fun = gen.GEN_loss()\n",
    "crossent_loss_fun = gen.GEN_loss_crossent()\n",
    "def vgae_loss(model, A, X, delta, Epsilon):\n",
    "    B = A + Epsilon\n",
    "    # delete all outgoing and incoming edges of deleted nodes\n",
    "    B[delta < -0.5, :] = 0\n",
    "    B[:, delta < -0.5] = 0\n",
    "    return model.compute_loss(torch.tensor(A, dtype=torch.float), torch.tensor(B, dtype=torch.float), torch.tensor(X, dtype=torch.float))\n",
    "def gen_loss_crossent(model, A, X, delta, Epsilon):\n",
    "    delta_pred, Epsilon_pred = model(torch.tensor(A, dtype=torch.float), torch.tensor(X, dtype=torch.float))\n",
    "    return crossent_loss_fun(delta_pred, Epsilon_pred, torch.tensor(delta, dtype=torch.float), torch.tensor(Epsilon, dtype=torch.float), torch.tensor(A, dtype=torch.float))\n",
    "def gen_loss(model, A, X, delta, Epsilon):\n",
    "    delta_pred, Epsilon_pred = model(torch.tensor(A, dtype=torch.float), torch.tensor(X, dtype=torch.float))\n",
    "    return loss_fun(delta_pred, Epsilon_pred, torch.tensor(delta, dtype=torch.float), torch.tensor(Epsilon, dtype=torch.float), torch.tensor(A, dtype=torch.float))\n",
    "loss_funs = [gen_loss, gen_loss_crossent, vgae_loss]\n",
    "# set up prediction functions\n",
    "def vgae_pred(model, A, X):\n",
    "    B = model(torch.tensor(A, dtype=torch.float), torch.tensor(X, dtype=torch.float))\n",
    "    B = B.detach().numpy()\n",
    "    Epsilon = B - A\n",
    "    delta = np.zeros(A.shape[0])\n",
    "    delta[np.sum(B, 1) < 0.5] = -1.\n",
    "    Epsilon[delta < -0.5, :] = 0.\n",
    "    Epsilon[:, delta < -0.5] = 0.\n",
    "    return delta, Epsilon\n",
    "def gen_pred(model, A, X):\n",
    "    delta_pred, Epsilon_pred = model(torch.tensor(A, dtype=torch.float), torch.tensor(X, dtype=torch.float))\n",
    "    delta_pred = delta_pred.detach().numpy()\n",
    "    Epsilon_pred = Epsilon_pred.detach().numpy()\n",
    "    delta = np.zeros(A.shape[0])\n",
    "    delta[delta_pred > 0.5] = 1.\n",
    "    delta[delta_pred < -0.5] = -1.\n",
    "    Epsilon = np.zeros(A.shape)\n",
    "    Epsilon[np.logical_and(A > 0.5, Epsilon_pred < -0.5)] = -1.\n",
    "    Epsilon[np.logical_and(A < 0.5, Epsilon_pred > +0.5)] = +1.\n",
    "    return delta, Epsilon\n",
    "pred_funs = [vgae_pred, gen_pred, gen_pred]\n",
    "\n",
    "eval_criteria = ['node_ins_recall',\n",
    "                 'node_ins_precision',\n",
    "                 'node_del_recall',\n",
    "                 'node_del_precision',\n",
    "                 'edge_ins_recall',\n",
    "                 'edge_ins_precision',\n",
    "                 'edge_del_recall',\n",
    "                 'edge_del_precision']\n",
    "# set up a function to compute precision and recall\n",
    "def prec_rec(X, Y):\n",
    "    # X is the prediction, Y is the target\n",
    "    target_insertions = Y > 0.5\n",
    "    predicted_insertions = X > 0.5\n",
    "    target_deletions = Y < -0.5\n",
    "    predicted_deletions = X < -0.5\n",
    "    # first, check the insertion recall\n",
    "    if np.sum(target_insertions) < 0.5:\n",
    "        ins_rec = 1.\n",
    "    else:\n",
    "        ins_rec  = np.mean(X[target_insertions] > 0.5)\n",
    "    # then the insertion precision\n",
    "    if np.sum(predicted_insertions) < 0.5:\n",
    "        ins_prec = 1.\n",
    "    else:\n",
    "        ins_prec = np.mean(Y[predicted_insertions] > 0.5)\n",
    "    # then the deletion recall\n",
    "    if np.sum(target_deletions) < 0.5:\n",
    "        del_rec = 1.\n",
    "    else:\n",
    "        del_rec  = np.mean(X[target_deletions] < -0.5)\n",
    "    # and finally the deletion precision\n",
    "    if np.sum(predicted_deletions) < 0.5:\n",
    "        del_prec = 1.\n",
    "    else:\n",
    "        del_prec = np.mean(Y[predicted_deletions] < -0.5)\n",
    "    return ins_rec, ins_prec, del_rec, del_prec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graph_edit_cycles\n",
    "import degree_rules\n",
    "import game_of_life\n",
    "import random\n",
    "\n",
    "datasets = ['edit_cycles', 'degree_rules', 'game_of_life']\n",
    "dim_ins  = [4, 32, 1]\n",
    "\n",
    "# set up a generative function for each data set\n",
    "def generate_edit_cycle():\n",
    "    As, Xs, tuples = graph_edit_cycles.generate_time_series(random.randrange(3), random.randrange(12), random.randrange(4, 12))\n",
    "    deltas = []\n",
    "    Epsilons = []\n",
    "    for tpl in tuples:\n",
    "        deltas.append(tpl[0])\n",
    "        Epsilons.append(tpl[1])\n",
    "    return As, Xs, deltas, Epsilons\n",
    "def generate_degree_rules():\n",
    "    # the initial number of nodes in each graph\n",
    "    n_init = 8\n",
    "    # the maximum number of nodes that can occur in each graph during evolution\n",
    "    n_max  = n_init * 4\n",
    "    return degree_rules.generate_time_series_from_random_matrix(n_init, n_max = n_max)\n",
    "def generate_game_of_life():\n",
    "    # set hyper-parameters for the game of life random grid generation\n",
    "    grid_size = 10\n",
    "    num_shapes = 1\n",
    "    p = 0.1\n",
    "    T_max = 10\n",
    "    A, Xs, deltas = game_of_life.generate_random_time_series(grid_size, num_shapes, p, T_max)\n",
    "    As = [A] * len(Xs)\n",
    "    Epsilons = [np.zeros_like(A)] * len(Xs)\n",
    "    return As, Xs, deltas, Epsilons\n",
    "generator_funs = [generate_edit_cycle, generate_degree_rules, generate_game_of_life]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- data set edit_cycles ---\n",
      "\n",
      "--- model GEN ---\n",
      "node_ins_recall: 0.636694 +- 0.0295303\n",
      "node_ins_precision: 1 +- 0\n",
      "node_del_recall: 1 +- 0\n",
      "node_del_precision: 0.748959 +- 0.104741\n",
      "edge_ins_recall: 1 +- 0\n",
      "edge_ins_precision: 1 +- 0\n",
      "edge_del_recall: 1 +- 0\n",
      "edge_del_precision: 1 +- 0\n",
      "--- model GEN_crossent ---\n",
      "-- repeat 1 of 5 --\n",
      "loss avg after 1000 epochs: 0.039952\n",
      "loss avg after 2000 epochs: 0.0335001\n",
      "loss avg after 3000 epochs: 0.037276\n",
      "loss avg after 4000 epochs: 0.045191\n",
      "loss avg after 5000 epochs: 0.037651\n",
      "loss avg after 6000 epochs: 0.0340091\n",
      "loss avg after 7000 epochs: 0.0255868\n",
      "loss avg after 8000 epochs: 0.0307316\n",
      "loss avg after 9000 epochs: 0.0299818\n",
      "loss avg after 10000 epochs: 0.0199267\n",
      "loss avg after 11000 epochs: 0.0217089\n",
      "loss avg after 12000 epochs: 0.0289838\n",
      "loss avg after 13000 epochs: 0.0300111\n",
      "loss avg after 14000 epochs: 0.0279755\n",
      "loss avg after 15000 epochs: 0.0275442\n",
      "loss avg after 16000 epochs: 0.0273392\n",
      "loss avg after 17000 epochs: 0.0202126\n",
      "loss avg after 18000 epochs: 0.020189\n",
      "loss avg after 19000 epochs: 0.0226794\n",
      "loss avg after 20000 epochs: 0.0303281\n",
      "loss avg after 21000 epochs: 0.0238554\n",
      "loss avg after 22000 epochs: 0.0247146\n",
      "loss avg after 23000 epochs: 0.028882\n",
      "loss avg after 24000 epochs: 0.0311972\n",
      "loss avg after 25000 epochs: 0.0294901\n",
      "loss avg after 26000 epochs: 0.0205316\n",
      "loss avg after 27000 epochs: 0.0247093\n",
      "loss avg after 28000 epochs: 0.031687\n",
      "loss avg after 29000 epochs: 0.0262136\n",
      "loss avg after 30000 epochs: 0.0369315\n",
      "loss avg after 31000 epochs: 0.0211264\n",
      "loss avg after 32000 epochs: 0.0182961\n",
      "loss avg after 33000 epochs: 0.0230959\n",
      "loss avg after 34000 epochs: 0.0241377\n",
      "loss avg after 35000 epochs: 0.0290347\n",
      "loss avg after 36000 epochs: 0.027004\n",
      "loss avg after 37000 epochs: 0.0257724\n",
      "loss avg after 38000 epochs: 0.027262\n",
      "loss avg after 39000 epochs: 0.0227199\n",
      "loss avg after 40000 epochs: 0.0242972\n",
      "loss avg after 41000 epochs: 0.02681\n",
      "loss avg after 42000 epochs: 0.0338585\n",
      "loss avg after 43000 epochs: 0.0216874\n",
      "loss avg after 44000 epochs: 0.0258786\n",
      "loss avg after 45000 epochs: 0.023509\n",
      "loss avg after 46000 epochs: 0.0259544\n",
      "loss avg after 47000 epochs: 0.0263755\n",
      "loss avg after 48000 epochs: 0.0293017\n",
      "loss avg after 49000 epochs: 0.0261649\n",
      "loss avg after 50000 epochs: 0.0256721\n",
      "-- repeat 2 of 5 --\n",
      "loss avg after 1000 epochs: 0.0397157\n",
      "loss avg after 2000 epochs: 0.0277139\n",
      "loss avg after 3000 epochs: 0.0302984\n",
      "loss avg after 4000 epochs: 0.030732\n",
      "loss avg after 5000 epochs: 0.0271965\n",
      "loss avg after 6000 epochs: 0.029029\n",
      "loss avg after 7000 epochs: 0.582218\n",
      "loss avg after 8000 epochs: 0.553426\n",
      "loss avg after 9000 epochs: 0.268117\n",
      "loss avg after 10000 epochs: 0.23966\n",
      "loss avg after 11000 epochs: 0.146006\n",
      "loss avg after 12000 epochs: 0.420081\n",
      "loss avg after 13000 epochs: 0.0436479\n",
      "loss avg after 14000 epochs: 0.0311466\n",
      "loss avg after 15000 epochs: 0.0271437\n",
      "loss avg after 16000 epochs: 0.0321847\n",
      "loss avg after 17000 epochs: 0.0182431\n",
      "loss avg after 18000 epochs: 0.0333182\n",
      "loss avg after 19000 epochs: 0.0319885\n",
      "loss avg after 20000 epochs: 0.035656\n",
      "loss avg after 21000 epochs: 0.0315048\n",
      "loss avg after 22000 epochs: 0.0254793\n",
      "loss avg after 23000 epochs: 0.0306201\n",
      "loss avg after 24000 epochs: 0.0262813\n",
      "loss avg after 25000 epochs: 0.0261004\n",
      "loss avg after 26000 epochs: 0.0256384\n",
      "loss avg after 27000 epochs: 0.0297129\n",
      "loss avg after 28000 epochs: 0.0378259\n",
      "loss avg after 29000 epochs: 0.0295932\n",
      "loss avg after 30000 epochs: 0.0271529\n",
      "loss avg after 31000 epochs: 0.0269897\n",
      "loss avg after 32000 epochs: 0.035007\n",
      "loss avg after 33000 epochs: 0.0316106\n",
      "loss avg after 34000 epochs: 0.0264023\n",
      "loss avg after 35000 epochs: 0.0310625\n",
      "loss avg after 36000 epochs: 0.0265702\n",
      "loss avg after 37000 epochs: 0.0236976\n",
      "loss avg after 38000 epochs: 0.0264036\n",
      "loss avg after 39000 epochs: 0.0268045\n",
      "loss avg after 40000 epochs: 0.0277901\n",
      "loss avg after 41000 epochs: 0.0295571\n",
      "loss avg after 42000 epochs: 0.0227951\n",
      "loss avg after 43000 epochs: 0.0342423\n",
      "loss avg after 44000 epochs: 0.0219015\n",
      "loss avg after 45000 epochs: 0.0236098\n",
      "loss avg after 46000 epochs: 0.0264976\n",
      "loss avg after 47000 epochs: 0.0279505\n",
      "loss avg after 48000 epochs: 0.0244696\n",
      "loss avg after 49000 epochs: 0.0202292\n",
      "loss avg after 50000 epochs: 0.0222906\n",
      "-- repeat 3 of 5 --\n",
      "loss avg after 1000 epochs: 0.0304473\n",
      "loss avg after 2000 epochs: 0.0271437\n",
      "loss avg after 3000 epochs: 0.0405286\n",
      "loss avg after 4000 epochs: 0.027434\n",
      "loss avg after 5000 epochs: 0.029149\n",
      "loss avg after 6000 epochs: 0.0248969\n",
      "loss avg after 7000 epochs: 0.0274382\n",
      "loss avg after 8000 epochs: 0.0235171\n",
      "loss avg after 9000 epochs: 0.0304305\n",
      "loss avg after 10000 epochs: 0.0237128\n",
      "loss avg after 11000 epochs: 0.0224101\n",
      "loss avg after 12000 epochs: 0.0252683\n",
      "loss avg after 13000 epochs: 0.0245344\n",
      "loss avg after 14000 epochs: 0.0276202\n",
      "loss avg after 15000 epochs: 0.0268574\n",
      "loss avg after 16000 epochs: 0.0210203\n",
      "loss avg after 17000 epochs: 0.0295162\n",
      "loss avg after 18000 epochs: 0.0263237\n",
      "loss avg after 19000 epochs: 0.0238043\n",
      "loss avg after 20000 epochs: 0.0288201\n",
      "loss avg after 21000 epochs: 0.0235646\n",
      "loss avg after 22000 epochs: 0.0243417\n",
      "loss avg after 23000 epochs: 0.032347\n",
      "loss avg after 24000 epochs: 0.0301854\n",
      "loss avg after 25000 epochs: 0.0241323\n",
      "loss avg after 26000 epochs: 0.0272294\n",
      "loss avg after 27000 epochs: 0.0207737\n",
      "loss avg after 28000 epochs: 0.0249604\n",
      "loss avg after 29000 epochs: 0.0216774\n",
      "loss avg after 30000 epochs: 0.0257496\n",
      "loss avg after 31000 epochs: 0.0351621\n",
      "loss avg after 32000 epochs: 0.0283462\n",
      "loss avg after 33000 epochs: 0.0288552\n",
      "loss avg after 34000 epochs: 0.022088\n",
      "loss avg after 35000 epochs: 0.0309026\n",
      "loss avg after 36000 epochs: 0.0284532\n",
      "loss avg after 37000 epochs: 0.0263665\n",
      "loss avg after 38000 epochs: 0.027241\n",
      "loss avg after 39000 epochs: 0.0255308\n",
      "loss avg after 40000 epochs: 0.0243888\n",
      "loss avg after 41000 epochs: 0.0252298\n",
      "loss avg after 42000 epochs: 0.0333613\n",
      "loss avg after 43000 epochs: 0.0302279\n",
      "loss avg after 44000 epochs: 0.0278144\n",
      "loss avg after 45000 epochs: 0.0262878\n",
      "loss avg after 46000 epochs: 0.0289033\n",
      "loss avg after 47000 epochs: 0.0302622\n",
      "loss avg after 48000 epochs: 0.0218994\n",
      "loss avg after 49000 epochs: 0.0246145\n",
      "loss avg after 50000 epochs: 0.0265153\n",
      "-- repeat 4 of 5 --\n",
      "loss avg after 1000 epochs: 0.051123\n",
      "loss avg after 2000 epochs: 0.0313084\n",
      "loss avg after 3000 epochs: 0.0289896\n",
      "loss avg after 4000 epochs: 0.0237043\n",
      "loss avg after 5000 epochs: 0.0335\n",
      "loss avg after 6000 epochs: 0.0291155\n",
      "loss avg after 7000 epochs: 0.0318797\n",
      "loss avg after 8000 epochs: 0.032316\n",
      "loss avg after 9000 epochs: 0.0311073\n",
      "loss avg after 10000 epochs: 0.0306074\n",
      "loss avg after 11000 epochs: 0.0184493\n",
      "loss avg after 12000 epochs: 0.0264977\n",
      "loss avg after 13000 epochs: 0.0265245\n",
      "loss avg after 14000 epochs: 0.0228078\n",
      "loss avg after 15000 epochs: 0.0250122\n",
      "loss avg after 16000 epochs: 0.0257744\n",
      "loss avg after 17000 epochs: 0.0255632\n",
      "loss avg after 18000 epochs: 0.02814\n",
      "loss avg after 19000 epochs: 0.0300119\n",
      "loss avg after 20000 epochs: 0.0250632\n",
      "loss avg after 21000 epochs: 0.0263645\n",
      "loss avg after 22000 epochs: 0.0289421\n",
      "loss avg after 23000 epochs: 0.0283456\n",
      "loss avg after 24000 epochs: 0.0234329\n",
      "loss avg after 25000 epochs: 0.0200733\n",
      "loss avg after 26000 epochs: 0.0218357\n",
      "loss avg after 27000 epochs: 0.0312878\n",
      "loss avg after 28000 epochs: 0.0255658\n",
      "loss avg after 29000 epochs: 0.0248136\n",
      "loss avg after 30000 epochs: 0.027246\n",
      "loss avg after 31000 epochs: 0.0205901\n",
      "loss avg after 32000 epochs: 0.0245725\n",
      "loss avg after 33000 epochs: 0.0288032\n",
      "loss avg after 34000 epochs: 0.0244402\n",
      "loss avg after 35000 epochs: 0.023066\n",
      "loss avg after 36000 epochs: 0.0230007\n",
      "loss avg after 37000 epochs: 0.0295039\n",
      "loss avg after 38000 epochs: 0.0288703\n",
      "loss avg after 39000 epochs: 0.0219461\n",
      "loss avg after 40000 epochs: 0.0240755\n",
      "loss avg after 41000 epochs: 0.025458\n",
      "loss avg after 42000 epochs: 0.0295817\n",
      "loss avg after 43000 epochs: 0.0244109\n",
      "loss avg after 44000 epochs: 0.0241768\n",
      "loss avg after 45000 epochs: 0.0222354\n",
      "loss avg after 46000 epochs: 0.0253765\n",
      "loss avg after 47000 epochs: 0.0284296\n",
      "loss avg after 48000 epochs: 0.020967\n",
      "loss avg after 49000 epochs: 0.025896\n",
      "loss avg after 50000 epochs: 0.0232095\n",
      "-- repeat 5 of 5 --\n",
      "loss avg after 1000 epochs: 0.0411761\n",
      "loss avg after 2000 epochs: 0.0256543\n",
      "loss avg after 3000 epochs: 0.0214571\n",
      "loss avg after 4000 epochs: 0.0253818\n",
      "loss avg after 5000 epochs: 0.0387019\n",
      "loss avg after 6000 epochs: 0.0236785\n",
      "loss avg after 7000 epochs: 0.0252071\n",
      "loss avg after 8000 epochs: 0.025945\n",
      "loss avg after 9000 epochs: 0.0228283\n",
      "loss avg after 10000 epochs: 0.0268062\n",
      "loss avg after 11000 epochs: 0.0225443\n",
      "loss avg after 12000 epochs: 0.0201784\n",
      "loss avg after 13000 epochs: 0.0250981\n",
      "loss avg after 14000 epochs: 0.0319112\n",
      "loss avg after 15000 epochs: 0.027992\n",
      "loss avg after 16000 epochs: 0.0279724\n",
      "loss avg after 17000 epochs: 0.071581\n",
      "loss avg after 18000 epochs: 0.0297137\n",
      "loss avg after 19000 epochs: 0.0282515\n",
      "loss avg after 20000 epochs: 0.0228619\n",
      "loss avg after 21000 epochs: 0.0276815\n",
      "loss avg after 22000 epochs: 0.0304822\n",
      "loss avg after 23000 epochs: 0.0289142\n",
      "loss avg after 24000 epochs: 0.0281179\n",
      "loss avg after 25000 epochs: 0.0274912\n",
      "loss avg after 26000 epochs: 0.0299198\n",
      "loss avg after 27000 epochs: 0.0279163\n",
      "loss avg after 28000 epochs: 0.0288432\n",
      "loss avg after 29000 epochs: 0.022618\n",
      "loss avg after 30000 epochs: 0.0298493\n",
      "loss avg after 31000 epochs: 0.022505\n",
      "loss avg after 32000 epochs: 0.0247004\n",
      "loss avg after 33000 epochs: 0.0286066\n",
      "loss avg after 34000 epochs: 0.0202314\n",
      "loss avg after 35000 epochs: 0.0192487\n",
      "loss avg after 36000 epochs: 0.0301852\n",
      "loss avg after 37000 epochs: 0.0298214\n",
      "loss avg after 38000 epochs: 0.0248817\n",
      "loss avg after 39000 epochs: 0.0245531\n",
      "loss avg after 40000 epochs: 0.0228091\n",
      "loss avg after 41000 epochs: 0.0233612\n",
      "loss avg after 42000 epochs: 0.0278785\n",
      "loss avg after 43000 epochs: 0.0261961\n",
      "loss avg after 44000 epochs: 0.0286286\n",
      "loss avg after 45000 epochs: 0.02356\n",
      "loss avg after 46000 epochs: 0.0250532\n",
      "loss avg after 47000 epochs: 0.0243056\n",
      "loss avg after 48000 epochs: 0.0241361\n",
      "loss avg after 49000 epochs: 0.0323224\n",
      "loss avg after 50000 epochs: 0.029796\n",
      "node_ins_recall: 1 +- 0\n",
      "node_ins_precision: 1 +- 0\n",
      "node_del_recall: 1 +- 0\n",
      "node_del_precision: 1 +- 0\n",
      "edge_ins_recall: 1 +- 0\n",
      "edge_ins_precision: 1 +- 0\n",
      "edge_del_recall: 1 +- 0\n",
      "edge_del_precision: 1 +- 0\n",
      "--- model VGAE ---\n",
      "-- repeat 1 of 5 --\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9e07ca69cb92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                     \u001b[1;31m# compute loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                     \u001b[0mloss_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_funs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEpsilons\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m                     \u001b[1;31m# compute gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                     \u001b[0mloss_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-8f60fb754bf5>\u001b[0m in \u001b[0;36mgen_loss\u001b[1;34m(model, A, X, delta, Epsilon)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcrossent_loss_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEpsilon_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEpsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgen_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEpsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mdelta_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEpsilon_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEpsilon_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEpsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mloss_funs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvgae_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_loss_crossent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_loss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "# iterate over all datasets\n",
    "for d in range(len(datasets)):\n",
    "    print('\\n--- data set %s ---\\n' % datasets[d])\n",
    "    # load partial runtime results if possible\n",
    "    runtimes_file = 'results/%s_runtimes.csv' % datasets[d]\n",
    "    if os.path.exists(runtimes_file):\n",
    "        runtimes = np.loadtxt(runtimes_file, skiprows = 1, delimiter = '\\t')\n",
    "    else:\n",
    "        runtimes = np.full((R, len(models)), np.nan)\n",
    "    # iterate over all models\n",
    "    for k in range(len(models)):\n",
    "        print('--- model %s ---' % models[k])\n",
    "        # load partial results if possible\n",
    "        results_file = 'results/%s_%s_results.csv' % (datasets[d], models[k])\n",
    "        curves_file  = 'results/%s_%s_learning_curves.csv' % (datasets[d], models[k])\n",
    "        if os.path.exists(results_file):\n",
    "            results = np.loadtxt(results_file, skiprows = 1, delimiter = '\\t')\n",
    "            learning_curves = np.loadtxt(curves_file, delimiter = '\\t')\n",
    "        else:\n",
    "            results = np.full((R, len(eval_criteria)), np.nan)\n",
    "            learning_curves = np.full((max_epochs, R), np.nan)\n",
    "        # iterate over experimental repeats\n",
    "        for r in range(R):\n",
    "            # check if this repeat is already evaluated; if so, skip it\n",
    "            if not np.isnan(learning_curves[0, r]):\n",
    "                continue\n",
    "            print('-- repeat %d of %d --' % (r+1, R))\n",
    "            start_time = time.time()\n",
    "            # set up model\n",
    "            if datasets[d] == 'game_of_life':\n",
    "                nonlin = torch.nn.Sigmoid()\n",
    "            else:\n",
    "                nonlin = torch.nn.ReLU()\n",
    "            model = setup_funs[k](dim_ins[d], nonlin)\n",
    "            # set up optimizer\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            # initialize moving loss average for printing\n",
    "            loss_avg = None\n",
    "            # start training\n",
    "            for epoch in range(max_epochs):\n",
    "                optimizer.zero_grad()\n",
    "                # sample a time series from the data set\n",
    "                As, Xs, deltas, Epsilons = generator_funs[d]()\n",
    "                # compute the loss over all time steps\n",
    "                loss = 0.\n",
    "                for t in range(len(As)):\n",
    "                    # compute loss\n",
    "                    loss_obj = loss_funs[k](model, As[t], Xs[t], deltas[t], Epsilons[t])\n",
    "                    # compute gradient\n",
    "                    loss_obj.backward()\n",
    "                    # accumulate loss\n",
    "                    loss += loss_obj.item()\n",
    "                # perform an optimizer step\n",
    "                optimizer.step()\n",
    "                # store the current loss value in the learning curve\n",
    "                learning_curves[epoch, r] = loss\n",
    "                # compute a new moving average over the loss\n",
    "                if loss_avg is None:\n",
    "                    loss_avg = loss\n",
    "                else:\n",
    "                    loss_avg = loss_avg * 0.9 + 0.1 * loss\n",
    "                # print every print_step steps\n",
    "                if(epoch+1) % print_step == 0:\n",
    "                    print('loss avg after %d epochs: %g' % (epoch+1, loss_avg))\n",
    "                # stop early if the moving average is small\n",
    "                if loss_avg < loss_threshold:\n",
    "                    break\n",
    "            # perform evaluation on new time series\n",
    "            results[r, :] = 0.\n",
    "            T = 0\n",
    "            for j in range(N_test):\n",
    "                # get a random time series from the dataset\n",
    "                As, Xs, deltas, Epsilons = generator_funs[d]()\n",
    "                for t in range(len(As)):\n",
    "                    # predict the current time step with the network\n",
    "                    delta, Epsilon = pred_funs[k](model, As[t], Xs[t])\n",
    "                    # assess node edit precision and recall\n",
    "                    results[r, :4] += prec_rec(delta, deltas[t])\n",
    "                    # assess edge edit precision and recall\n",
    "                    results[r, 4:] += prec_rec(Epsilon, Epsilons[t])\n",
    "                        \n",
    "                T += len(As)\n",
    "            results[r, :] /= T\n",
    "            # store runtime\n",
    "            runtimes[r, k] = time.time() - start_time\n",
    "            np.savetxt(runtimes_file, runtimes, delimiter = '\\t', fmt = '%g', header = '\\t'.join(models), comments = '')\n",
    "            # store results\n",
    "            np.savetxt(results_file, results, delimiter = '\\t', fmt = '%g', header = '\\t'.join(eval_criteria), comments = '')\n",
    "            # store learning curves\n",
    "            np.savetxt(curves_file, learning_curves, delimiter = '\\t', fmt = '%g')\n",
    "        # print results\n",
    "        for crit in range(len(eval_criteria)):\n",
    "            print('%s: %g +- %g' % (eval_criteria[crit], np.mean(results[:, crit]), np.std(results[:, crit])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize learning curves\n",
    "import matplotlib.pyplot as plt\n",
    "smoothing_steps = 10\n",
    "fig, axes = plt.subplots(ncols=1, nrows=len(datasets))\n",
    "for d in range(len(datasets)):\n",
    "    for k in range(len(models)):\n",
    "        curves_file  = 'results/%s_%s_learning_curves.csv' % (datasets[d], models[k])\n",
    "        learning_curves = np.loadtxt(curves_file, delimiter = '\\t')\n",
    "        acum = np.cumsum(np.nanmean(learning_curves, 1))\n",
    "        axes[d].semilogy((acum[smoothing_steps:] - acum[:-smoothing_steps])/smoothing_steps)\n",
    "    axes[d].set_xlabel('epoch')\n",
    "    axes[d].set_ylabel('loss')\n",
    "    axes[d].set_title(datasets[d])\n",
    "    axes[d].legend(models)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
